{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Installation of all dependencies:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: langchain in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: langchain_cohere in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: langchain_groq in c:\\users\\user\\anaconda3\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: ollama in c:\\users\\user\\anaconda3\\lib\\site-packages (0.3.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.0.25)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.5.14)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.3.21)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (0.1.134)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.5.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (0.3.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: cohere<6.0,>=5.5.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (5.11.0)\n",
      "Requirement already satisfied: langchain-experimental>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (0.3.3)\n",
      "Requirement already satisfied: pandas>=1.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (2.2.3)\n",
      "Requirement already satisfied: tabulate<0.10.0,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: groq<1,>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain_groq) (0.11.0)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from ollama) (0.27.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.3)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.34.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.35.38)\n",
      "Requirement already satisfied: fastavro<2.0.0,>=1.9.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (1.9.7)\n",
      "Requirement already satisfied: parameterized<0.10.0,>=0.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (0.9.0)\n",
      "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.23.4)\n",
      "Requirement already satisfied: sagemaker<3.0.0,>=2.232.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.232.2)\n",
      "Requirement already satisfied: types-requests<3.0.0,>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (2.32.0.20240914)\n",
      "Requirement already satisfied: typing_extensions>=4.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from cohere<6.0,>=5.5.6->langchain_cohere) (4.12.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.8.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\user\\anaconda3\\lib\\site-packages (from groq<1,>=0.4.1->langchain_groq) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\user\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.17->langchain_community) (1.33)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas>=1.4.3->langchain_cohere) (2023.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain_community) (3.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.38 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.35.38)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from boto3<2.0.0,>=1.34.0->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.17->langchain_community) (2.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.3->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.2.1)\n",
      "Requirement already satisfied: docker in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.19.2)\n",
      "Requirement already satisfied: pathos in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.3.6)\n",
      "Collecting protobuf<5.0,>=3.12 (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere)\n",
      "  Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (5.9.0)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.1.0)\n",
      "Requirement already satisfied: schema in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.7.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.21.0)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jsonschema->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.10.6)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\user\\anaconda3\\lib\\site-packages (from docker->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (305.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.7.6.9)\n",
      "Collecting dill>=0.3.9 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pox>=0.3.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.3.5)\n",
      "Collecting multiprocess>=0.70.17 (from pathos->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere)\n",
      "  Using cached multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: mlflow>=2.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.16.2)\n",
      "Requirement already satisfied: mlflow-skinny==2.16.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.16.2)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.3.2)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.13.3)\n",
      "Requirement already satisfied: graphene<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.4.1)\n",
      "Requirement already satisfied: matplotlib<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.8.0)\n",
      "Collecting pyarrow<18,>=4.0.0 (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.5.2)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.13.1)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.3)\n",
      "Requirement already satisfied: waitress<4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.0)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.34.0)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.37)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.28.1)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.28.1)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.5.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.18.0)\n",
      "Requirement already satisfied: Mako in c:\\users\\user\\anaconda3\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.3.5)\n",
      "Requirement already satisfied: Werkzeug>=2.3.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.1.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.6.2)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.2.4)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.1.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.0.9)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (3.5.0)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (2.36.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.7)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.49b1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (1.16.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.2->mlflow>=2.8->sagemaker-mlflow->sagemaker<3.0.0,>=2.232.1->cohere<6.0,>=5.5.6->langchain_cohere) (0.6.1)\n",
      "Using cached protobuf-4.25.5-cp310-abi3-win_amd64.whl (413 kB)\n",
      "Using cached dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/25.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/25.2 MB 817.9 kB/s eta 0:00:30\n",
      "   - -------------------------------------- 1.0/25.2 MB 948.7 kB/s eta 0:00:26\n",
      "   - -------------------------------------- 1.0/25.2 MB 948.7 kB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/25.2 MB 963.8 kB/s eta 0:00:25\n",
      "   -- ------------------------------------- 1.8/25.2 MB 1.1 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 2.1/25.2 MB 1.0 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 2.1/25.2 MB 1.0 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 2.4/25.2 MB 1.0 MB/s eta 0:00:23\n",
      "   ---- ----------------------------------- 2.9/25.2 MB 1.1 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 3.1/25.2 MB 1.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 1.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 3.4/25.2 MB 1.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 3.7/25.2 MB 1.1 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 3.7/25.2 MB 1.1 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.0 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.0 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 3.9/25.2 MB 1.0 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 4.2/25.2 MB 953.2 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 4.5/25.2 MB 945.1 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 4.7/25.2 MB 950.6 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 4.7/25.2 MB 950.6 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 5.0/25.2 MB 937.7 kB/s eta 0:00:22\n",
      "   ------- -------------------------------- 5.0/25.2 MB 937.7 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 5.2/25.2 MB 923.9 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 5.2/25.2 MB 923.9 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 5.5/25.2 MB 897.1 kB/s eta 0:00:22\n",
      "   -------- ------------------------------- 5.5/25.2 MB 897.1 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 5.8/25.2 MB 882.9 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 6.0/25.2 MB 876.6 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 6.0/25.2 MB 876.6 kB/s eta 0:00:22\n",
      "   --------- ------------------------------ 6.0/25.2 MB 876.6 kB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 6.3/25.2 MB 842.4 kB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 6.3/25.2 MB 842.4 kB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 6.6/25.2 MB 838.9 kB/s eta 0:00:23\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 842.1 kB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 6.8/25.2 MB 842.1 kB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 7.1/25.2 MB 842.1 kB/s eta 0:00:22\n",
      "   ----------- ---------------------------- 7.3/25.2 MB 848.3 kB/s eta 0:00:21\n",
      "   ------------ --------------------------- 7.6/25.2 MB 851.0 kB/s eta 0:00:21\n",
      "   ------------ --------------------------- 7.6/25.2 MB 851.0 kB/s eta 0:00:21\n",
      "   ------------ --------------------------- 7.9/25.2 MB 855.0 kB/s eta 0:00:21\n",
      "   ------------ --------------------------- 8.1/25.2 MB 853.1 kB/s eta 0:00:20\n",
      "   ------------ --------------------------- 8.1/25.2 MB 853.1 kB/s eta 0:00:20\n",
      "   ------------- -------------------------- 8.4/25.2 MB 851.2 kB/s eta 0:00:20\n",
      "   ------------- -------------------------- 8.7/25.2 MB 849.5 kB/s eta 0:00:20\n",
      "   ------------- -------------------------- 8.7/25.2 MB 849.5 kB/s eta 0:00:20\n",
      "   -------------- ------------------------- 8.9/25.2 MB 846.5 kB/s eta 0:00:20\n",
      "   -------------- ------------------------- 9.2/25.2 MB 851.4 kB/s eta 0:00:19\n",
      "   -------------- ------------------------- 9.2/25.2 MB 851.4 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 9.4/25.2 MB 848.5 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 9.7/25.2 MB 847.1 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 9.7/25.2 MB 847.1 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 10.0/25.2 MB 842.2 kB/s eta 0:00:19\n",
      "   --------------- ------------------------ 10.0/25.2 MB 842.2 kB/s eta 0:00:19\n",
      "   ---------------- ----------------------- 10.2/25.2 MB 843.3 kB/s eta 0:00:18\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 843.2 kB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 10.7/25.2 MB 844.1 kB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 10.7/25.2 MB 844.1 kB/s eta 0:00:18\n",
      "   ----------------- ---------------------- 11.0/25.2 MB 847.1 kB/s eta 0:00:17\n",
      "   ----------------- ---------------------- 11.3/25.2 MB 849.9 kB/s eta 0:00:17\n",
      "   ------------------ --------------------- 11.5/25.2 MB 849.7 kB/s eta 0:00:17\n",
      "   ------------------ --------------------- 11.5/25.2 MB 849.7 kB/s eta 0:00:17\n",
      "   ------------------ --------------------- 11.8/25.2 MB 854.4 kB/s eta 0:00:16\n",
      "   ------------------- -------------------- 12.1/25.2 MB 858.9 kB/s eta 0:00:16\n",
      "   ------------------- -------------------- 12.3/25.2 MB 865.2 kB/s eta 0:00:15\n",
      "   -------------------- ------------------- 12.6/25.2 MB 870.3 kB/s eta 0:00:15\n",
      "   -------------------- ------------------- 12.6/25.2 MB 870.3 kB/s eta 0:00:15\n",
      "   -------------------- ------------------- 12.8/25.2 MB 863.1 kB/s eta 0:00:15\n",
      "   -------------------- ------------------- 13.1/25.2 MB 868.1 kB/s eta 0:00:14\n",
      "   -------------------- ------------------- 13.1/25.2 MB 868.1 kB/s eta 0:00:14\n",
      "   -------------------- ------------------- 13.1/25.2 MB 868.1 kB/s eta 0:00:14\n",
      "   --------------------- ------------------ 13.4/25.2 MB 849.0 kB/s eta 0:00:14\n",
      "   --------------------- ------------------ 13.6/25.2 MB 853.1 kB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 13.9/25.2 MB 854.4 kB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 13.9/25.2 MB 854.4 kB/s eta 0:00:14\n",
      "   ---------------------- ----------------- 14.2/25.2 MB 853.3 kB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 854.7 kB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 14.4/25.2 MB 854.7 kB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 14.7/25.2 MB 850.4 kB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 14.7/25.2 MB 850.4 kB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 14.9/25.2 MB 847.9 kB/s eta 0:00:13\n",
      "   ----------------------- ---------------- 14.9/25.2 MB 847.9 kB/s eta 0:00:13\n",
      "   ------------------------ --------------- 15.2/25.2 MB 844.0 kB/s eta 0:00:12\n",
      "   ------------------------ --------------- 15.5/25.2 MB 844.7 kB/s eta 0:00:12\n",
      "   ------------------------- -------------- 15.7/25.2 MB 846.0 kB/s eta 0:00:12\n",
      "   ------------------------- -------------- 15.7/25.2 MB 846.0 kB/s eta 0:00:12\n",
      "   ------------------------- -------------- 16.0/25.2 MB 842.4 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 16.0/25.2 MB 842.4 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 16.0/25.2 MB 842.4 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 16.3/25.2 MB 834.1 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 16.3/25.2 MB 834.1 kB/s eta 0:00:11\n",
      "   ------------------------- -------------- 16.3/25.2 MB 834.1 kB/s eta 0:00:11\n",
      "   -------------------------- ------------- 16.5/25.2 MB 821.0 kB/s eta 0:00:11\n",
      "   -------------------------- ------------- 16.5/25.2 MB 821.0 kB/s eta 0:00:11\n",
      "   -------------------------- ------------- 16.8/25.2 MB 817.4 kB/s eta 0:00:11\n",
      "   --------------------------- ------------ 17.0/25.2 MB 817.8 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 17.0/25.2 MB 817.8 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 17.3/25.2 MB 817.5 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 17.3/25.2 MB 817.5 kB/s eta 0:00:10\n",
      "   --------------------------- ------------ 17.6/25.2 MB 814.2 kB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 17.8/25.2 MB 813.4 kB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 17.8/25.2 MB 813.4 kB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 18.1/25.2 MB 813.7 kB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 18.1/25.2 MB 813.7 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 811.2 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 18.4/25.2 MB 811.2 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 806.6 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 806.6 kB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 18.6/25.2 MB 806.6 kB/s eta 0:00:09\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 18.9/25.2 MB 793.6 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.1/25.2 MB 765.0 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.1/25.2 MB 765.0 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.1/25.2 MB 765.0 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.4/25.2 MB 756.9 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.4/25.2 MB 756.9 kB/s eta 0:00:08\n",
      "   ------------------------------ --------- 19.4/25.2 MB 756.9 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.7/25.2 MB 749.2 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.7/25.2 MB 749.2 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.7/25.2 MB 749.2 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.9/25.2 MB 744.1 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.9/25.2 MB 744.1 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.9/25.2 MB 744.1 kB/s eta 0:00:08\n",
      "   ------------------------------- -------- 19.9/25.2 MB 744.1 kB/s eta 0:00:08\n",
      "   -------------------------------- ------- 20.2/25.2 MB 726.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.2/25.2 MB 726.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.2/25.2 MB 726.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.2/25.2 MB 726.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.4/25.2 MB 717.3 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.4/25.2 MB 717.3 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.4/25.2 MB 717.3 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.7/25.2 MB 708.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.7/25.2 MB 708.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 20.7/25.2 MB 708.5 kB/s eta 0:00:07\n",
      "   --------------------------------- ------ 21.0/25.2 MB 705.7 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.0/25.2 MB 705.7 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.0/25.2 MB 705.7 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.2/25.2 MB 700.5 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.2/25.2 MB 700.5 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.2/25.2 MB 700.5 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.2/25.2 MB 700.5 kB/s eta 0:00:06\n",
      "   --------------------------------- ------ 21.2/25.2 MB 700.5 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.5/25.2 MB 677.4 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.5/25.2 MB 677.4 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.5/25.2 MB 677.4 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.8/25.2 MB 655.4 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.8/25.2 MB 655.4 kB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 21.8/25.2 MB 655.4 kB/s eta 0:00:06\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 639.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 639.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 639.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.0/25.2 MB 639.5 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 621.4 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 621.4 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.3/25.2 MB 621.4 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.5/25.2 MB 620.4 kB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 22.5/25.2 MB 620.4 kB/s eta 0:00:05\n",
      "   ------------------------------------ --- 22.8/25.2 MB 615.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.8/25.2 MB 615.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 22.8/25.2 MB 615.2 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 23.1/25.2 MB 607.0 kB/s eta 0:00:04\n",
      "   ------------------------------------ --- 23.1/25.2 MB 607.0 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 23.3/25.2 MB 604.2 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 23.3/25.2 MB 604.2 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 23.3/25.2 MB 604.2 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 23.6/25.2 MB 600.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.6/25.2 MB 600.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.6/25.2 MB 600.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.9/25.2 MB 595.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.9/25.2 MB 595.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.9/25.2 MB 595.1 kB/s eta 0:00:03\n",
      "   ------------------------------------- -- 23.9/25.2 MB 595.1 kB/s eta 0:00:03\n",
      "   -------------------------------------- - 24.1/25.2 MB 588.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.2 MB 588.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.2 MB 588.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.1/25.2 MB 588.8 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.2 MB 569.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.2 MB 569.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.2 MB 569.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.2 MB 569.2 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 24.4/25.2 MB 569.2 kB/s eta 0:00:02\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.6/25.2 MB 547.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 518.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 518.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 518.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 518.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------  24.9/25.2 MB 518.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 499.4 kB/s eta 0:00:00\n",
      "Installing collected packages: pyarrow, protobuf, dill, multiprocess\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 18.0.0\n",
      "    Uninstalling pyarrow-18.0.0:\n",
      "      Successfully uninstalled pyarrow-18.0.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.28.3\n",
      "    Uninstalling protobuf-5.28.3:\n",
      "      Successfully uninstalled protobuf-5.28.3\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.8\n",
      "    Uninstalling dill-0.3.8:\n",
      "      Successfully uninstalled dill-0.3.8\n",
      "  Attempting uninstall: multiprocess\n",
      "    Found existing installation: multiprocess 0.70.16\n",
      "    Uninstalling multiprocess-0.70.16:\n",
      "      Successfully uninstalled multiprocess-0.70.16\n",
      "Successfully installed dill-0.3.9 multiprocess-0.70.17 protobuf-4.25.5 pyarrow-17.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.1.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n",
      "datasets 3.1.0 requires multiprocess<0.70.17, but you have multiprocess 0.70.17 which is incompatible.\n",
      "opentelemetry-proto 1.28.1 requires protobuf<6.0,>=5.0, but you have protobuf 4.25.5 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community langchain langchain_cohere langchain_groq langchain ollama transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import langchain\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain_groq import ChatGroq  \n",
    "from langchain.llms import Ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LLM (Llama3-8b-8192 / Llama3.2-3b)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "GROQ_API_KEY='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'\n",
    "os.environ[\"GROQ_API_KEY\"]='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm= ChatGroq(\n",
    "            model='llama3-8b-8192',\n",
    "            temperature=0.6,\n",
    "            api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2952\\4284971636.py:1: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm =Ollama(model='llama3.2:3b')\n"
     ]
    }
   ],
   "source": [
    "llm =Ollama(model='llama3.2:3b') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing LLMs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Programming! It's like trying to teach a computer to do a magic trick, but instead of making a rabbit appear, it's making your program appear to work correctly.\\n\\nWhy did the programmer quit his job? Because he didn't get arrays! (get a raise)\\n\\nBut seriously, programming is all about writing instructions that a computer can understand, using languages like Python, Java, or C++. It's like writing a recipe for your computer, telling it what to do step by step.\\n\\nHere's a joke to help you understand variables:\\n\\nWhy did the variable go to therapy? Because it was feeling undefined!\\n\\nAnd here's one about loops:\\n\\nWhy did the programmer break up with his girlfriend? Because he wanted to loop through all the other girls!\\n\\nProgramming can be challenging, but it's also super rewarding when you finally get something to work. And who knows, maybe one day you'll create the next big app or game!\\n\\nWant to hear another joke?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 193, 'prompt_tokens': 28, 'total_tokens': 221, 'completion_time': 0.160833333, 'prompt_time': 0.00361768, 'queue_time': 0.011082257, 'total_time': 0.164451013}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None} id='run-57afe6e1-3ca7-4c68-b8b3-186311ce8528-0' usage_metadata={'input_tokens': 28, 'output_tokens': 193, 'total_tokens': 221}\n",
      "Programming! It's like trying to solve a puzzle blindfolded while being attacked by a swarm of bees!\n",
      "\n",
      "But seriously, programming is like building with Legos, but instead of blocks, you use lines of code. You need to connect the right pieces in the right order to create something amazing!\n",
      "\n",
      "Here's a joke to help you understand the concept of debugging:\n",
      "\n",
      "Why did the programmer quit his job?\n",
      "\n",
      "Because he didn't get arrays! (get a raise)\n",
      "\n",
      "And here's another one:\n",
      "\n",
      "Why do programmers prefer dark mode?\n",
      "\n",
      "Because light attracts bugs!\n",
      "\n",
      "Okay, okay, one more:\n",
      "\n",
      "Why did the programmer go to the doctor?\n",
      "\n",
      "Because he was feeling a little glitchy!\n",
      "\n",
      "I hope these jokes helped ease the pain of understanding programming a bit!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that tells jokes.\"),\n",
    "    HumanMessage(content=\"Tell me about programming\")\n",
    "]\n",
    "response = llm.invoke(messages)\n",
    "print(response)\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant that tells jokes.\"),\n",
    "    (\"human\", \"Tell me about {topic}\")\n",
    "])\n",
    "chain = template | llm|StrOutputParser()\n",
    "response = chain.invoke({\"topic\": \"programming\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Doccument Processing For RAG***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File upload "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents from the folder.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "def load_documents(folder_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if filename.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(file_path)\n",
    "        elif filename.endswith('.docx'):\n",
    "            loader = Docx2txtLoader(file_path)\n",
    "        else:\n",
    "            print(f\"Unsupported file type: {filename}\")\n",
    "            continue\n",
    "        documents.extend(loader.load())\n",
    "    return documents\n",
    "\n",
    "folder_path = r\"C:\\Users\\user\\Desktop\\Alchemist AI Assignment\\data\"\n",
    "documents = load_documents(folder_path)\n",
    "print(f\"Loaded {len(documents)} documents from the folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chunking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split the documents into 115 chunks.\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=10,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Split the documents into {len(splits)} chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='dynasty, ruled over the Indian subcontinent from' metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}\n"
     ]
    }
   ],
   "source": [
    "print(splits[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\ProgramData\\sagemaker\\sagemaker\\config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: C:\\Users\\user\\AppData\\Local\\sagemaker\\sagemaker\\config.yaml\n"
     ]
    }
   ],
   "source": [
    "#Cohere Embeddings\n",
    "\n",
    "embeddings_model_name='embed-english-v2.0'\n",
    "COHERE_API_KEY='30lAzH7VO1y9M00rSD5ns4gIIxicqxiMnCn1IycB'\n",
    "import os\n",
    "os.environ['COHERE_API_KEY']=COHERE_API_KEY \n",
    "from langchain_cohere import CohereEmbeddings\n",
    "\n",
    "embeddings = CohereEmbeddings(\n",
    "        model=embeddings_model_name,\n",
    "        cohere_api_key=os.getenv('COHERE_API_KEY')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2464\\154022173.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Sentence Transformer Based Embeddings\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msentence_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerEmbeddings\n\u001b[1;32m----> 5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m SentenceTransformerEmbeddings(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m document_embeddings \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39membed_documents([split\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits])\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(document_embeddings[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:216\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     emit_warning()\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\huggingface.py:84\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     warn_deprecated(\n\u001b[0;32m     75\u001b[0m         since\u001b[38;5;241m=\u001b[39msince,\n\u001b[0;32m     76\u001b[0m         removal\u001b[38;5;241m=\u001b[39mremoval,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m constructor instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     81\u001b[0m     )\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import sentence_transformers python package. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install it with `pip install sentence-transformers`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     90\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\__init__.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\backend.py:11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Callable, Literal\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_datasets_caching, is_datasets_available\n\u001b[0;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\sentence_transformers\\util.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download, snapshot_download\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, device\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:2665\u001b[0m\n\u001b[0;32m   2661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTORCH_DEVICE_BACKEND_AUTOLOAD\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_device_backend_autoload_enabled():\n\u001b[1;32m-> 2665\u001b[0m     _import_device_backends()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\torch\\__init__.py:2632\u001b[0m, in \u001b[0;36m_import_device_backends\u001b[1;34m()\u001b[0m\n\u001b[0;32m   2630\u001b[0m     backend_extensions \u001b[38;5;241m=\u001b[39m entry_points()\u001b[38;5;241m.\u001b[39mget(group_name, ())\n\u001b[0;32m   2631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2632\u001b[0m     backend_extensions \u001b[38;5;241m=\u001b[39m entry_points(group\u001b[38;5;241m=\u001b[39mgroup_name)\n\u001b[0;32m   2634\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m backend_extension \u001b[38;5;129;01min\u001b[39;00m backend_extensions:\n\u001b[0;32m   2635\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2636\u001b[0m         \u001b[38;5;66;03m# Load the extension\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:1040\u001b[0m, in \u001b[0;36mentry_points\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m:return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   1038\u001b[0m     dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1039\u001b[0m )\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m SelectableGroups\u001b[38;5;241m.\u001b[39mload(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:476\u001b[0m, in \u001b[0;36mSelectableGroups.load\u001b[1;34m(cls, eps)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, eps):\n\u001b[0;32m    475\u001b[0m     by_group \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 476\u001b[0m     ordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(eps, key\u001b[38;5;241m=\u001b[39mby_group)\n\u001b[0;32m    477\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(ordered, by_group)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((group, EntryPoints(eps)) \u001b[38;5;28;01mfor\u001b[39;00m group, eps \u001b[38;5;129;01min\u001b[39;00m grouped)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\importlib\\metadata\\__init__.py:1038\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[EntryPoints, SelectableGroups]:\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    :return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 1038\u001b[0m         dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1039\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SelectableGroups\u001b[38;5;241m.\u001b[39mload(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\importlib_metadata\\__init__.py:491\u001b[0m, in \u001b[0;36mDistribution.entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    489\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m EntryPoints:\n\u001b[1;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints\u001b[38;5;241m.\u001b[39m_from_text_for(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_text(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry_points.txt\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\importlib_metadata\\__init__.py:856\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename: StrPath) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    850\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    851\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    855\u001b[0m     ):\n\u001b[1;32m--> 856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;241m.\u001b[39mjoinpath(filename)\u001b[38;5;241m.\u001b[39mread_text(encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    858\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\pathlib.py:1058\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopen(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "File \u001b[1;32m<frozen codecs>:309\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, errors)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Sentence Transformer Based Embeddings\n",
    "\n",
    "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "document_embeddings = embeddings.embed_documents([split.page_content for split in splits])\n",
    "print(document_embeddings[0][:5])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CohereEmbeddings(client=<cohere.client.Client object at 0x0000020A40F4D950>, async_client=<cohere.client.AsyncClient object at 0x0000020A42BC4A90>, model='embed-english-v2.0', truncate=None, cohere_api_key=SecretStr('**********'), embedding_types=['float'], max_retries=3, request_timeout=None, user_agent='langchain:partner', base_url=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vector Database Creation,Ingestion:ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pypika.dialects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections: []\n"
     ]
    }
   ],
   "source": [
    "from chromadb.config import Settings\n",
    "from chromadb import Client\n",
    "\n",
    "# Initialize ChromaDB with persistence\n",
    "settings = Settings(persist_directory=\"./chroma_db\")\n",
    "client = Client(settings=settings)\n",
    "\n",
    "# List available collections\n",
    "try:\n",
    "    collections = client.list_collections()\n",
    "    print(f\"Collections: {collections}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error listing collections: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection created: Collection(name=my_collection)\n"
     ]
    }
   ],
   "source": [
    "# Create or get a collection\n",
    "try:\n",
    "    collection = client.get_or_create_collection(name=\"my_collection\")\n",
    "    print(\"Collection created:\", collection)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating collection: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created and persisted to './chroma_db'\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "collection_name=\"my_collection\"\n",
    "vectorstore = Chroma.from_documents(\n",
    "    collection_name=collection_name,\n",
    "    documents=splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n",
    "\n",
    "print(\"Vector store created and persisted to './chroma_db'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 2 most relevant chunks for the query: 'What was babar?'\n",
      "\n",
      "Result 1:\n",
      "Source: C:\\Users\\user\\Desktop\\Alchemist AI Assignment\\data\\The Mughal Empire.pdf\n",
      "Content: Babur's grandson, Akbar the Great, is often\n",
      "\n",
      "Result 2:\n",
      "Source: C:\\Users\\user\\Desktop\\Alchemist AI Assignment\\data\\The Mughal Empire.pdf\n",
      "Content: Babur's grandson, Akbar the Great, is often\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What was babar?\"\n",
    "search_results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTop 2 most relevant chunks for the query: '{query}'\\n\")\n",
    "for i, result in enumerate(search_results, 1):\n",
    "    print(f\"Result {i}:\")\n",
    "    print(f\"Source: {result.metadata.get('source', 'Unknown')}\")\n",
    "    print(f\"Content: {result.page_content}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normal Retriever\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "# retriever_results = retriever.invoke(\"Who was Akbar?\")\n",
    "# print(retriever_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_10756\\1361929948.py:8: LangChainDeprecationWarning: The class `CohereRerank` was deprecated in LangChain 0.0.30 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-cohere package and should be used instead. To use it run `pip install -U :class:`~langchain-cohere` and import as `from :class:`~langchain_cohere import CohereRerank``.\n",
      "  compressor = CohereRerank(cohere_api_key=os.getenv('COHERE_API_KEY'))\n"
     ]
    }
   ],
   "source": [
    "#Contextual Compression Retriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "#from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "#Compression\n",
    "#Compressior=LLMChainExtractor.from_llm(llm)\n",
    "compressor = CohereRerank(cohere_api_key=os.getenv('COHERE_API_KEY'))\n",
    "#Contextual Compression\n",
    "retriever=ContextualCompressionRetriever(base_compressor=compressor,\n",
    "                                         base_retriever=retriever)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Simple RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def docs2str(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | docs2str, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was Akbar?\n",
      "Answer: Akbar the Great\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was Akbar?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Who was his son?\n",
      "Answer: Aurangzeb\n"
     ]
    }
   ],
   "source": [
    "question = \"Who was his son?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was his religious policy?\n",
      "Answer: The answer is: not mentioned. The context only mentions that Aurangzeb expanded the empire, but does not mention the religious policy of the father.\n"
     ]
    }
   ],
   "source": [
    "question = \"What was his religious policy?\"\n",
    "response = rag_chain.invoke(question)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History Aware Retriver based RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "Given a chat history and the latest user question\n",
    "which might reference context in the chat history,\n",
    "formulate a standalone question which can be understood\n",
    "without the chat history. Do NOT answer the question,\n",
    "just reformulate it if needed and otherwise return it as is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_chain = contextualize_q_prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who was Akbar, the Mughal Emperor?\n"
     ]
    }
   ],
   "source": [
    "print(contextualize_chain.invoke({\"input\": \"Who was Akbar?\", \"chat_history\": []}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI assistant. Use the following context to answer the user's question.Answer to the point and precise manner.\"),\n",
    "    (\"system\", \"Context: {context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**History Aware RAG chain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who was Akbar?\n",
      "AI: Akbar was the grandson of Babur and the third Mughal Emperor of India. He is often credited with consolidating and expanding the Mughal Empire.\n",
      "\n",
      "Human: Who was his Son?\n",
      "AI: Akbar's son was Jahangir, who succeeded him as the fourth Mughal Emperor of India.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = []\n",
    "question1 = \"Who was Akbar?\"\n",
    "answer1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question1),\n",
    "    AIMessage(content=answer1)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question1}\")\n",
    "print(f\"AI: {answer1}\\n\")\n",
    "\n",
    "question2 = \"Who was his Son?\"\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=question2),\n",
    "    AIMessage(content=answer2)\n",
    "])\n",
    "\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining Every Block of Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_groq import ChatGroq\n",
    "from typing import List\n",
    "\n",
    "def create_chains(llm, retriever):\n",
    "    \"\"\"Create necessary chains for the RAG pipeline.\"\"\"\n",
    "    contextualize_ques_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        Given a chat history and the latest user question\n",
    "        which might reference context in the chat history,\n",
    "        formulate a standalone question which can be understood\n",
    "        without the chat history. Do NOT answer the question,\n",
    "        just reformulate it if needed and otherwise return it as is.\n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    contextualize_chain = contextualize_ques_prompt | llm | StrOutputParser()\n",
    "    history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_ques_prompt)\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        You are a helpful RAG AI assistant. \n",
    "        Use the following context to answer the user's question. \n",
    "        Answer to the point,precise and in a very specific manner.\n",
    "        If it is an out-of-context question, do not answer and say it is out of context.\n",
    "        \"\"\"),\n",
    "        (\"system\", \"Context: {context}\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "    return rag_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "def create_generic_prompt_chain(llm):\n",
    "    \"\"\"Create a chain to handle vague/generic inputs using FewShotPromptTemplate.\"\"\"\n",
    "    \n",
    "    # Define a set of examples for FewShotPromptTemplate\n",
    "    examples = [\n",
    "    {\n",
    "        \"input\": \"Who was Akbar?\",\n",
    "        \"output\": \"Akbar was the grandson of Babur and a great Mughal emperor.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Tell me about Mughal empire.\",\n",
    "        \"output\": \"Would you like to know more about Akbar or explore other aspects of the Mughal empire's history and achievements?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Explain photosynthesis.\",\n",
    "        \"output\": \"Photosynthesis is the process by which plants convert sunlight into energy. Would you like details on the chemical process, its stages, or its significance in ecosystems?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"What are planets?\",\n",
    "        \"output\": \"Planets are celestial bodies orbiting stars. Are you asking about their formation, characteristics, or specific ones like Earth or Mars?\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Tell me about Mars.\",\n",
    "        \"output\": \"Mars is known as the Red Planet due to its iron oxide surface. Would you like to learn about its atmosphere, exploration missions, or potential for life?\"\n",
    "    }\n",
    "]\n",
    "    \n",
    "    # Example template for FewShotPromptTemplate\n",
    "    example_template = \"\"\"\n",
    "    User: {input}\n",
    "    AI: {output}\n",
    "    \"\"\"\n",
    "    example_prompt = PromptTemplate.from_template(example_template)\n",
    "    \n",
    "    # FewShotPromptTemplate\n",
    "    generic_prompt = FewShotPromptTemplate(\n",
    "        examples=examples,\n",
    "        example_prompt=example_prompt,\n",
    "        prefix=\"The following is a conversation between the AI and the user. The user's inputs are vague or generic. Use the chat history to understand the context and guide the user to provide more specific input.\",\n",
    "        suffix=\"Chat History:\\n{chat_history}\\nUser: {input}\\nAI:\",\n",
    "        input_variables=[\"chat_history\", \"input\"]\n",
    "    )\n",
    "    \n",
    "    # Create the chain\n",
    "    generic_chain =  generic_prompt | llm\n",
    "    return generic_chain\n",
    " # Replace with your LLM instance\n",
    "# generic_chain = create_generic_prompt_chain(llm)\n",
    "\n",
    "# # Test the chain with a query\n",
    "# query = \"Tell me about Mughal empire.\"\n",
    "# response = generic_chain.invoke(input=query)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generic_prompt_chain(llm):\n",
    "    \"\"\"Create a chain to handle vague/generic inputs using ChatPromptTemplate.\"\"\"\n",
    "    \n",
    "    # Define a system message for context\n",
    "    generic_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"\n",
    "        The user's inputs are vague or generic. \n",
    "        Use the chat history to understand the context and guide the user \n",
    "        to provide more specific input.Don't answer the query but ask follow up question mainting the context .\n",
    "        examples:\n",
    "        \"input\": \"Tell me about Mars.\",\n",
    "        \"output\": \"Mars is known as the Red Planet due to its iron oxide surface. Would you like to learn about its atmosphere, exploration missions, or potential for life?\"\n",
    "    \n",
    "        \"input\": \"What are planets?\",\n",
    "        \"output\": \"Planets are celestial bodies orbiting stars. Are you asking about their formation, characteristics, or specific ones like Earth or Mars?\"\n",
    "     \n",
    "        \"\"\"),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    # Combine the prompt and LLM\n",
    "    generic_chain = generic_prompt | llm\n",
    "    return generic_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_k_messages(chat_history, k=1):\n",
    "    \"\"\"Get the last k messages from the chat history.\"\"\"\n",
    "    return chat_history[-k:] if len(chat_history) > k else chat_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "\n",
    "def router_logic(llm, user_input):\n",
    "    \"\"\"Decides if the input is 'specific' or 'generic'.\"\"\"\n",
    "    router_prompt = PromptTemplate(\n",
    "        input_variables=[\"input\"],\n",
    "        template=\"\"\"\n",
    "        Based on the input provided, classify it as either 'specific' or 'generic'.\n",
    "        Input: {input}\n",
    "        Respond with either 'specific' or 'generic' only.\n",
    "        \"\"\"\n",
    "    )\n",
    "    router_chain = router_prompt | llm\n",
    "    classification = router_chain.invoke({\"input\": user_input})\n",
    "    return classification.content.strip().lower()\n",
    "\n",
    "def create_router_chain(llm, retriever,last_k_chat_history, user_input):\n",
    "    \"\"\"Combines RAG and generic chains with routing logic and returns the final answer.\"\"\"\n",
    "    # Create the history-aware RAG chain\n",
    "    rag_chain = create_chains(llm, retriever)\n",
    "\n",
    "    # Create the generic chain\n",
    "    generic_chain = create_generic_prompt_chain(llm)\n",
    "    \n",
    "    # Determine whether input is 'specific' or 'generic'\n",
    "    classification = router_logic(llm, user_input)\n",
    "    print('classification: ',classification)\n",
    "    if classification == \"specific\":\n",
    "        # History-aware invocation for the RAG chain\n",
    "        print('rag chain invoked')\n",
    "        response = rag_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": last_k_chat_history\n",
    "        })['answer']\n",
    "    elif classification == \"generic\":\n",
    "        # Direct invocation of the generic chain\n",
    "        print('generic chain invoked')\n",
    "        response = generic_chain.invoke({\n",
    "            \"input\": user_input,\n",
    "            \"chat_history\": last_k_chat_history\n",
    "        })\n",
    "    else:\n",
    "        # Fallback for unclassified cases\n",
    "        response = \"I'm sorry, I couldn't classify your input. Please try again.\"\n",
    "\n",
    "    return response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification:  specific\n",
      "rag chain invoked\n",
      "Babur was a descendant of Timur and Genghis Khan, and the founder of the Mughal Empire in India. He was a Central Asian ruler who conquered India in the early 16th century and established the Mughal Empire.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = \"who was babur?.\"\n",
    "response = create_router_chain(llm, retriever,chat_history, user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification:  generic\n",
      "generic chain invoked\n",
      "content='Jahangir! He was quite the interesting character. As the son of Akbar, he inherited the throne of the Mughal Empire, but his reign was marked by turmoil and power struggles. Did you know that Jahangir was known for his love of art, architecture, and literature? He even wrote his own poetry and was a patron of the arts.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 254, 'total_tokens': 330, 'completion_time': 0.063333333, 'prompt_time': 0.030050287, 'queue_time': 0.002686758000000001, 'total_time': 0.09338362}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None} id='run-7baf71b0-8da7-4efb-aef7-63dee8c60a68-0' usage_metadata={'input_tokens': 254, 'output_tokens': 76, 'total_tokens': 330}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "user_input = \"Tell me anything interesting.\"\n",
    "response = create_router_chain(llm, retriever,chat_history,user_input)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Up SQLite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "DB_NAME = \"rag_app.db\"\n",
    "\n",
    "def get_db_connection():\n",
    "    conn = sqlite3.connect(DB_NAME)\n",
    "    conn.row_factory = sqlite3.Row\n",
    "    return conn\n",
    "\n",
    "def create_application_logs():\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('''CREATE TABLE IF NOT EXISTS application_logs\n",
    "    (id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    session_id TEXT,\n",
    "    user_query TEXT,\n",
    "    gpt_response TEXT,\n",
    "    model TEXT,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)''')\n",
    "    conn.close()\n",
    "\n",
    "def insert_application_logs(session_id, user_query, gpt_response, model):\n",
    "    conn = get_db_connection()\n",
    "    conn.execute('INSERT INTO application_logs (session_id, user_query, gpt_response, model) VALUES (?, ?, ?, ?)',\n",
    "                 (session_id, user_query, gpt_response, model))\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def get_chat_history(session_id):\n",
    "    conn = get_db_connection()\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT user_query, gpt_response FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))\n",
    "    messages = []\n",
    "    for row in cursor.fetchall():\n",
    "        messages.extend([\n",
    "            {\"role\": \"human\", \"content\": row['user_query']},\n",
    "            {\"role\": \"ai\", \"content\": row['gpt_response']}\n",
    "        ])\n",
    "    conn.close()\n",
    "    return messages\n",
    "\n",
    "# Initialize the database\n",
    "create_application_logs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Who made Taj Mahal?\n",
      "AI: The Taj Mahal was commissioned by Shah Jahan, the grandson of Akbar the Great, in memory of his beloved wife, Mumtaz Mahal.\n",
      "\n",
      "Human: Who was Babur?\n",
      "AI: Babur was the founder of the Mughal Empire in India. He was a descendant of Timur (also known as Tamerlane) and Genghis Khan, and was a Central Asian ruler who conquered India and established the Mughal Empire in 1526 after his victory at the Battle of Panipat.\n"
     ]
    }
   ],
   "source": [
    "# Example usage for a new user\n",
    "session_id = str(uuid.uuid4())\n",
    "question = \"Who made Taj Mahal?\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question, answer, \"llama3-8b\")\n",
    "print(f\"Human: {question}\")\n",
    "print(f\"AI: {answer}\\n\")\n",
    "\n",
    "# Example of a follow-up question\n",
    "question2 = \"Who was Babur?\"\n",
    "chat_history = get_chat_history(session_id)\n",
    "answer2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})['answer']\n",
    "insert_application_logs(session_id, question2, answer2, \"llama3-8b\")\n",
    "print(f\"Human: {question2}\")\n",
    "print(f\"AI: {answer2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAGAS setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -q langchain  ragas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context. If you cannot answer the question with the context, please respond with 'I don't know':\n",
    "\n",
    "### CONTEXT\n",
    "{context}\n",
    "\n",
    "### QUESTION\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "primary_qa_llm = llm\n",
    "\n",
    "rag_chain = (\n",
    "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
    "    # \"question\" : populated by getting the value of the \"question\" key\n",
    "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
    "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
    "    #              by getting the value of the \"context\" key from the previous step\n",
    "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
    "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
    "    #              into the LLM and stored in a key called \"response\"\n",
    "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
    "    | {\"response\": prompt | primary_qa_llm, \"context\": itemgetter(\"context\")}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "question_schema = ResponseSchema(\n",
    "    name=\"question\",\n",
    "    description=\"a question about the context.\"\n",
    ")\n",
    "\n",
    "question_response_schemas = [\n",
    "    question_schema,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_output_parser = StructuredOutputParser.from_response_schemas(question_response_schemas)\n",
    "format_instructions = question_output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_llm = llm\n",
    "\n",
    "bare_prompt_template = \"{content}\"\n",
    "bare_template = ChatPromptTemplate.from_template(template=bare_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. \n",
    "For each context, create a question that is specific to the context.\n",
    "Avoid creating generic or general questions.\n",
    "\n",
    "question: a question about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "question\n",
    "\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=splits[0],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "question_generation_chain = bare_template | question_generation_llm\n",
    "\n",
    "response = question_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = question_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "What was the primary source of revenue for the Mughal Empire during its peak in the 17th century?\n",
      "context\n",
      "{'page_content': 'The Mughal Empire, a prominent and influential', 'metadata': {'source': 'C:\\\\\\\\Users\\\\\\\\user\\\\\\\\Desktop\\\\\\\\Alchemist AI Assignment\\\\\\\\data\\\\\\\\The Mughal Empire.pdf', 'page': 0}}\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:07<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "qac_triples = []\n",
    "\n",
    "for text in tqdm(splits[:7]):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=text,\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = question_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = question_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  output_dict[\"context\"] = text\n",
    "  qac_triples.append(output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What was the name of the dynasty that ruled over the Indian subcontinent from 1526 to 1756, and who was its founder?',\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='dynasty, ruled over the Indian subcontinent from')},\n",
       " {'question': \"What is the significance of the Mughal Empire's administrative capital, Fatehpur Sikri, in the context of Akbar's reign and the empire's expansion?\",\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='from the')},\n",
       " {'question': 'What was the primary source of revenue for the Mughal Empire during the early 16th century, as mentioned in the text?',\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='early 16th century until the mid-18th century,')},\n",
       " {'question': 'What was the primary reason for the decline of the Mughal Empire in the 18th century, despite its remnants lasting until the mid-19th century?',\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='century, with remnants lasting until the mid-19th')},\n",
       " {'question': \"What was the primary source of revenue for the Mughal Empire during the mid-19th century, according to the text on page 0 of 'The Mughal Empire'?\",\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='mid-19th century. The')},\n",
       " {'question': 'What was the name of the founder who established the Mughal Empire in 1526?',\n",
       "  'context': Document(metadata={'source': 'C:\\\\Users\\\\user\\\\Desktop\\\\Alchemist AI Assignment\\\\data\\\\The Mughal Empire.pdf', 'page': 0}, page_content='empire was established in 1526 by Babur, a')}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qac_triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_generation_llm = llm\n",
    "\n",
    "answer_schema = ResponseSchema(\n",
    "    name=\"answer\",\n",
    "    description=\"an answer to the question\"\n",
    ")\n",
    "\n",
    "answer_response_schemas = [\n",
    "    answer_schema,\n",
    "]\n",
    "\n",
    "answer_output_parser = StructuredOutputParser.from_response_schemas(answer_response_schemas)\n",
    "format_instructions = answer_output_parser.get_format_instructions()\n",
    "\n",
    "qa_template = \"\"\"\\\n",
    "You are a University Professor creating a test for advanced students. For each question and context, create an answer.\n",
    "\n",
    "answer: a answer about the context.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "answer\n",
    "\n",
    "question: {question}\n",
    "context: {context}\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template=qa_template)\n",
    "\n",
    "messages = prompt_template.format_messages(\n",
    "    context=qac_triples[0][\"context\"],\n",
    "    question=qac_triples[0][\"question\"],\n",
    "    format_instructions=format_instructions\n",
    ")\n",
    "\n",
    "answer_generation_chain = bare_template | answer_generation_llm\n",
    "\n",
    "response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "output_dict = answer_output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "The Mughal Empire, founded by Babur\n"
     ]
    }
   ],
   "source": [
    "for k, v in output_dict.items():\n",
    "  print(k)\n",
    "  print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:05<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for triple in tqdm(qac_triples):\n",
    "  messages = prompt_template.format_messages(\n",
    "      context=triple[\"context\"],\n",
    "      question=triple[\"question\"],\n",
    "      format_instructions=format_instructions\n",
    "  )\n",
    "  response = answer_generation_chain.invoke({\"content\" : messages})\n",
    "  try:\n",
    "    output_dict = answer_output_parser.parse(response.content)\n",
    "  except Exception as e:\n",
    "    continue\n",
    "  triple[\"answer\"] = output_dict[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "ground_truth_qac_set = pd.DataFrame(qac_triples)\n",
    "ground_truth_qac_set[\"context\"] = ground_truth_qac_set[\"context\"].map(lambda x: str(x.page_content))\n",
    "ground_truth_qac_set = ground_truth_qac_set.rename(columns={\"answer\" : \"ground_truth\"})\n",
    "\n",
    "\n",
    "eval_dataset = Dataset.from_pandas(ground_truth_qac_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0260ed70c0314a56b0b972bd7b037df2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1949"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset.to_csv(\"groundtruth_eval_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'context', 'ground_truth'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    # context_recall,\n",
    "    # answer_correctness,\n",
    "    # answer_similarity\n",
    ")\n",
    "\n",
    "\n",
    "#from ragas.metrics.critique import harmfulness\n",
    "from ragas import evaluate\n",
    "\n",
    "def create_ragas_dataset(rag_pipeline, eval_dataset):\n",
    "  rag_dataset = []\n",
    "  for row in tqdm(eval_dataset):\n",
    "    answer = rag_pipeline.invoke({\"question\" : row[\"question\"]})\n",
    "    rag_dataset.append(\n",
    "        {\"question\" : row[\"question\"],\n",
    "          \"answer\" : answer[\"response\"].content,\n",
    "          \"contexts\" : [context.page_content for context in answer[\"context\"]],\n",
    "          \"ground_truths\" : [row[\"ground_truth\"]]\n",
    "          }\n",
    "    )\n",
    "  rag_df = pd.DataFrame(rag_dataset)\n",
    "  rag_eval_dataset = Dataset.from_pandas(rag_df)\n",
    "  return rag_eval_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the name of the dynasty that ruled over the Indian subcontinent from 1526 to 1756, and who was its founder?\n",
      "What is the significance of the Mughal Empire's administrative capital, Fatehpur Sikri, in the context of Akbar's reign and the empire's expansion?\n",
      "What was the primary source of revenue for the Mughal Empire during the early 16th century, as mentioned in the text?\n",
      "What was the primary reason for the decline of the Mughal Empire in the 18th century, despite its remnants lasting until the mid-19th century?\n",
      "What was the primary source of revenue for the Mughal Empire during the mid-19th century, according to the text on page 0 of 'The Mughal Empire'?\n",
      "What was the name of the founder who established the Mughal Empire in 1526?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for row in tqdm(eval_dataset):\n",
    "        # Ensure the question is a string\n",
    "        question = str(row[\"question\"])\n",
    "        print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:14<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "basic_qa_ragas_dataset = create_ragas_dataset(rag_chain, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'contexts', 'ground_truths'],\n",
       "    num_rows: 6\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87600dcdb404397839f3231815d3acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2858"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_ragas_dataset.to_csv(\"basic_qa_ragas_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ragas_dataset(ragas_dataset):\n",
    "  result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=[\n",
    "        faithfulness,\n",
    "        answer_relevancy,\n",
    "        # context_recall,\n",
    "        # answer_correctness,\n",
    "        # answer_similarity\n",
    "    ]\n",
    "  )\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d72f217bc1641d38b7b9da992781e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception raised in Job[9]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[3]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[5]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[1]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[2]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[6]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[4]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[7]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[10]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[11]: RateLimitError(Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}})\n",
      "Exception raised in Job[0]: TimeoutError()\n",
      "Exception raised in Job[8]: TimeoutError()\n"
     ]
    }
   ],
   "source": [
    "basic_qa_result = evaluate_ragas_dataset(basic_qa_ragas_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'faithfulness': nan, 'answer_relevancy': nan}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_qa_result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
